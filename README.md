Here I want to share the implementation of some of my university projects. Initially, each of them was about working with data using specialized software (Gephi, Orange, Excel) and conducting research. Python was only partially used. Nevertheless, it was decided to adapt each of these projects into a kind of jupyter notebook, comparing the results with those previously obtained. Let's briefly describe each of the projects: 

1) Clustering of Kinopoisk series: the project was made in December 2021 as part of the course Instrumental Methods for Digital Economics. At that time, we were studying the use of various algorithms (K-Means, DBSCAN, Hierarchical Clustering, Louvain Clustering), using Orange for data clustering. As a homework assignment, we were offered to create our own clustering mini project, which has its own history, structure, and goals. As a topic, I chose a possible solution of a problem that I wanted to solve for some time - how to logically choose a TV series to maintain spoken English, based on preferences?

2) Convex Optimization: about 2.5 years ago (beginning of 2020) I became interested in Probability Theory and Statistics. After learning the basics, I wanted to understand how these methods are used to solve practical problems. In this regard, at the university I chose a project on Stochastic Models. The supervisor suggested moving towards the intersection of the fields of Machine Learning and Stochastic Processes. Having studied the literature on Stochastic Gradient Descent, I realized that different studies focus on completely different aspects of this vast topic. In this regard, I decided to write a research term paper, where I tried to summarize the results of numerous studies, giving both mathematical and intuitive justification of algorithms, as well as Python code with examples and analysis. The final version was done in June 2022 and now I am planning to convert this into an article. In the future, I plan to use the obtained results to optimize neural networks

3) Community Detection: This project was done in November 2021 as part of the course Instrumental Methods for Digital Economics, where we were taught Network Analysis and optimization on graphs using Gephi. This time, as part of our homework, we were asked to build a network on our own, calculate the centrality and modularity indices, as well as identify communities and interpret the results in a meaningful way. My plan was to come up with a possible solution of the problem of asymmetry of information between employees and employers. For example, a vacancy presupposes the presence of a technical specialty. Can Economics, for example, be considered a technical specialty? The answer is ambiguous, since it depends on the university and faculty, so it seems useful to have a generalized model that can assess the similarities of certain faculties based on the subjects studied

4) Comparison of Ensemble Learners: In the summer of this year (2022), I began to be interested in advanced and relatively new Machine Learning algorithms, such as XGBoost, CatBoost and LightGBM. At the same time, these algorithms are a kind of final stages of evolution, which began with simple Decision Trees. Trees, despite their speed and logic, are quite subjective algorithms with a large variance. Therefore, people have come up with various ways to improve these algorithms through the use of Ensemble Learning methods. I decided to compare (in terms of speed and accuracy) the most popular of them by solving the regression problem (predicting salaries of Data Science specialists)

5) Classification using ML and DL: I wanted to solve at least one classification problem as a project. At the same time, classical classification is quite boring, and the technique is not much different from the regression task that we did before. Therefore, it was decided to try to classify the text. A fairly small number of Machine Learning algorithms are able to process text attributes (as far as I know, Naive Bayes and SVM Classifier are most often used for such tasks). At the same time, the researchers claim that Neural Networks are very good at processing text features and doing classification based on them. Therefore, using the example of a dataset with descriptions of wines, as well as their countries, I decided to compare the efficiency of classical Machine Learning algorithms with Neural Networks in terms of speed and accuracy

6) Clicks prediction Kaggle: As a homework for Large Scale Machine Learning course I got a task to do a binary classification problem. Initially, this task was a competition on Kaggle. The dataset consisted of several tables, which are totally over 100GB uncompressed. It was an interesting experience, because I had to pay a lot of attention to get useful features by merging different tables with each other, as well as study insights from people who got top scores. With the use of modern methods of processing Big Data (PySpark) and parallel model training (Vowpal Wabbit), we were able to get a fairly good score. 
